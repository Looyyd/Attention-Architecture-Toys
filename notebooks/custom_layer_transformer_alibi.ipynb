{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-24T13:40:02.043300Z",
     "start_time": "2023-06-24T13:39:58.742407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple implementation of a custom layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import Model, layers, Input\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
    "\n",
    "# pad the sequences to the same length\n",
    "x_train = pad_sequences(x_train, maxlen=200)\n",
    "x_test = pad_sequences(x_test, maxlen=200)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T13:40:04.127026Z",
     "start_time": "2023-06-24T13:40:02.043608Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    <a id=\"PrepareMHA\"></a>\n",
    "\n",
    "    ## Prepare for multi-head attention\n",
    "\n",
    "    This module does a linear transformation and splits the vector into given\n",
    "    number of heads for multi-head attention.\n",
    "    This is used to transform **key**, **query**, and **value** vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        # Linear layer for linear transform\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "        # Number of dimensions in vectors in each head\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`.\n",
    "        # We apply the linear transformation to the last dimension and split that into\n",
    "        # the heads.\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        # Linear transform\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Split last dimension into heads\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "\n",
    "        # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, heads, d_model]`\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    r\"\"\"\n",
    "    <a id=\"MHA\"></a>\n",
    "\n",
    "    ## Multi-Head Attention Module\n",
    "\n",
    "    This computes scaled multi-headed attention for given `query`, `key` and `value` vectors.\n",
    "\n",
    "    $$\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n",
    "\n",
    "    In simple terms, it finds keys that matches the query, and gets the values of\n",
    "     those keys.\n",
    "\n",
    "    It uses dot-product of query and key as the indicator of how matching they are.\n",
    "    Before taking the $softmax$ the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$.\n",
    "    This is done to avoid large dot-product values causing softmax to\n",
    "    give very small gradients when $d_k$ is large.\n",
    "\n",
    "    Softmax is calculated along the axis of of the sequence (or time).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        \"\"\"\n",
    "        * `heads` is the number of heads.\n",
    "        * `d_model` is the number of features in the `query`, `key` and `value` vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of features per head\n",
    "        self.d_k = d_model // heads\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "\n",
    "        # These transform the `query`, `key` and `value` vectors for multi-headed attention.\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "\n",
    "        # Softmax for attention along the time dimension of `key`\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Scaling factor before the softmax\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "        # We store attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ### Calculate scores between queries and keys\n",
    "\n",
    "        This method can be overridden for other variations like relative attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate $Q K^\\top$ or $S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}$\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "\n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        \"\"\"\n",
    "        `mask` has shape `[seq_len_q, seq_len_k, batch_size]`, where first dimension is the query dimension.\n",
    "        If the query dimension is equal to $1$ it will be broadcasted.\n",
    "        \"\"\"\n",
    "\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        # Same mask applied to all heads.\n",
    "        mask = mask.unsqueeze(-1)\n",
    "\n",
    "        # resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`\n",
    "        return mask\n",
    "\n",
    "    def forward(self, *,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        `query`, `key` and `value` are the tensors that store\n",
    "        collection of *query*, *key* and *value* vectors.\n",
    "        They have shape `[seq_len, batch_size, d_model]`.\n",
    "\n",
    "        `mask` has shape `[seq_len, seq_len, batch_size]` and\n",
    "        `mask[i, j, b]` indicates whether for batch `b`,\n",
    "        query at position `i` has access to key-value at position `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        # Prepare `query`, `key` and `value` for attention computation.\n",
    "        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute attention scores $Q K^\\top$.\n",
    "        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n",
    "        scores = self.get_scores(query, key)\n",
    "\n",
    "        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        scores *= self.scale\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # $softmax$ attention along the key sequence dimension\n",
    "        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Save attentions if debugging\n",
    "\n",
    "        # Apply dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Multiply by values\n",
    "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Save attentions for any other calculations\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        # Output layer\n",
    "        return self.output(x)\n",
    "\n",
    "def get_slopes(n_heads: int):\n",
    "    \"\"\"\n",
    "    ## Get head-specific slope $m$ for each head\n",
    "\n",
    "    * `n_heads` is the number of heads in the attention layer $n$\n",
    "\n",
    "    The slope for first head is\n",
    "\n",
    "    $$\\frac{1}{2^{\\frac{8}{n}}} = 2^{-\\frac{8}{n}}$$\n",
    "\n",
    "    The slopes for the rest of the heads are in a geometric series with a ratio same as above.\n",
    "\n",
    "    For instance when the number of heads is $8$ the slopes are\n",
    "    $$\\frac{1}{2^1}, \\frac{1}{2^2}, \\dots, \\frac{1}{2^8}$$\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the closest power of 2 to `n_heads`.\n",
    "    # If `n_heads` is not a power of 2, then we first calculate slopes to the closest (smaller) power of 2,\n",
    "    # and then add the remaining slopes.\n",
    "    n = 2 ** math.floor(math.log2(n_heads))\n",
    "    # $2^{-\\frac{8}{n}}$\n",
    "    m_0 = 2.0 ** (-8.0 / n)\n",
    "    # $2^{-1\\frac{8}{n}}, 2^{-2 \\frac{8}{n}}, 2^{-3 \\frac{8}{n}}, \\dots$\n",
    "    m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "\n",
    "    # If `n_heads` is not a power of 2, then we add the remaining slopes.\n",
    "    # We calculate the remaining slopes for $n * 2$ (avoiding slopes added previously).\n",
    "    # And pick the slopes upto `n_heads`.\n",
    "    if n < n_heads:\n",
    "        # $2^{-\\frac{8}{2n}}$\n",
    "        m_hat_0 = 2.0 ** (-4.0 / n)\n",
    "        # $2^{-1\\frac{8}{2n}}, 2^{-3 \\frac{8}{2n}}, 2^{-5 \\frac{8}{2n}}, \\dots$\n",
    "        # Note that we take steps by $2$ to avoid slopes added previously.\n",
    "        m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (n_heads - n), 2))\n",
    "        # Concatenate the slopes with the remaining slopes.\n",
    "        m = torch.cat([m, m_hat])\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_alibi_biases(n_heads: int, mask: torch.Tensor):\n",
    "    \"\"\"\n",
    "    ## Calculate the attention biases matrix\n",
    "\n",
    "    * `n_heads` is the number of heads in the attention layer\n",
    "    * `mask` is the attention mask of shape `[seq_len_q, seq_len_k]`\n",
    "\n",
    "    This returns a matrix of shape `[seq_len_q, seq_len_k, n_heads, ]` with ALiBi attention biases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get slopes $m$ for each head\n",
    "    m = get_slopes(n_heads).to(mask.device)\n",
    "\n",
    "    # Calculate distances $[0, 1, \\dots, N]$\n",
    "    # Here we calculate the distances using the mask.\n",
    "    #\n",
    "    # Since it's causal mask we can just use $[0, 1, \\dots, N]$ too.\n",
    "    # `distance = torch.arange(mask.shape[1], dtype=torch.long, device=mask.device)[None, :]`\n",
    "    distance = mask.cumsum(dim=-1)\n",
    "\n",
    "    # Multiply them pair-wise to get the AliBi bias matrix\n",
    "    return distance[:, :, None] * m[None, None, :]\n",
    "\n",
    "\n",
    "class AlibiMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    ## Attention with Linear Biases (ALiBi)\n",
    "\n",
    "    We override [Multi-Head Attention](../mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__(heads, d_model, dropout_prob)\n",
    "\n",
    "        # To cache AliBi the biases\n",
    "        self.alibi_biases = None\n",
    "\n",
    "    def forward(self, *,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        `query`, `key` and `value` are the tensors that store\n",
    "        collection of *query*, *key* and *value* vectors.\n",
    "        They have shape `[seq_len, batch_size, d_model]`.\n",
    "\n",
    "        `mask` has shape `[seq_len, seq_len, batch_size]` and\n",
    "        `mask[i, j, b]` indicates whether for batch `b`,\n",
    "        query at position `i` has access to key-value at position `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # ALiBi only works with causal masks.\n",
    "        assert mask is not None\n",
    "        assert mask.shape[0] == mask.shape[1] and mask.shape[2] == 1\n",
    "\n",
    "        # `query`, `key` and `value` have shape `[seq_len, batch_size, d_model]`\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        # Add head dimension to mask and check its shape.\n",
    "        mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        # Prepare `query`, `key` and `value` for attention computation.\n",
    "        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute attention scores $Q K^\\top$.\n",
    "        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n",
    "        scores = self.get_scores(query, key)\n",
    "\n",
    "        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        scores *= self.scale\n",
    "\n",
    "        # Create AliBi biases if it's not cached\n",
    "        if self.alibi_biases is None or self.alibi_biases.shape[1] < seq_len:\n",
    "            # `mask` has shape [seq_len, seq_len, 1, 1]\n",
    "            self.alibi_biases = get_alibi_biases(scores.shape[-1], mask[:, :, 0, 0])\n",
    "\n",
    "        # Add AliBi biases to attention scores.\n",
    "        # ALiBi biases has shape `[seq_len, seq_len, n_heads]`\n",
    "        # and `scores` has shape `[seq_len, seq_len, batch_size, n_heads]`\n",
    "        scores += self.alibi_biases[:seq_len, :seq_len, None, :]\n",
    "\n",
    "        # Apply mask\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # $softmax$ attention along the key sequence dimension\n",
    "        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Apply dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Multiply by values\n",
    "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        # Output layer\n",
    "        return self.output(x)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T13:40:04.138243Z",
     "start_time": "2023-06-24T13:40:04.136691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AlibiMultiHeadAttention.forward() missing 3 required keyword-only arguments: 'query', 'key', and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m inputs \u001B[38;5;241m=\u001B[39m Input(shape\u001B[38;5;241m=\u001B[39m(max_sequence_length,))\n\u001B[1;32m     12\u001B[0m embedding_layer \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mEmbedding(vocab_size, embed_dim)(inputs)\n\u001B[0;32m---> 13\u001B[0m attention_layer \u001B[38;5;241m=\u001B[39m \u001B[43mAlibiMultiHeadAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m pooling_layer \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mGlobalAveragePooling1D()(attention_layer)\n\u001B[1;32m     15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m)(pooling_layer)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/LanguageDetection/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: AlibiMultiHeadAttention.forward() missing 3 required keyword-only arguments: 'query', 'key', and 'value'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "# Find the maximum word index in x_train and x_test\n",
    "max_word_index = max(np.max(x_train), np.max(x_test))\n",
    "# Set vocab_size to be at least as large as max_word_index + 1\n",
    "vocab_size = max_word_index + 1  # Add 1 because word indices start from 1\n",
    "max_sequence_length = x_train.shape[1]  # The length of each sequence\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "attention_layer = AlibiMultiHeadAttention(num_heads, embed_dim)()\n",
    "pooling_layer = layers.GlobalAveragePooling1D()(attention_layer)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T13:45:36.757367Z",
     "start_time": "2023-06-24T13:45:36.728569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loss of 0.0788\n",
    "# Evaluating the model\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "# 782/782 [==============================] - 6s 7ms/step - loss: 0.3520 - accuracy: 0.8654"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model 2 with positional embedding\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        angles = self.get_angles(np.arange(sequence_length)[:, np.newaxis],\n",
    "                                 np.arange(output_dim)[np.newaxis, :],\n",
    "                                 output_dim)\n",
    "        self.positional_encoding = tf.cast(angles, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_encoding[:shape[-2], :shape[-1]]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "# Find the maximum word index in x_train and x_test\n",
    "max_word_index = max(np.max(x_train), np.max(x_test))\n",
    "# Set vocab_size to be at least as large as max_word_index + 1\n",
    "vocab_size = max_word_index + 1  # Add 1 because word indices start from 1\n",
    "max_sequence_length = x_train.shape[1]  # The length of each sequence\n",
    "\n",
    "# Define the model\n",
    "inputs = layers.Input(shape=(max_sequence_length,))\n",
    "embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "position_embedding_layer = PositionalEncoding(max_sequence_length, embed_dim)(embedding_layer)\n",
    "attention_layer = AlibiMultiHeadAttention(num_heads, embed_dim)(position_embedding_layer)\n",
    "pooling_layer = layers.GlobalAveragePooling1D()(attention_layer)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model2\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model2\n",
    "model2.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model 2 with positional embedding evaluation:\n",
    "model2.evaluate(x_test, y_test)\n",
    "\n",
    "# 2 epoch:\n",
    "# 782/782 [==============================] - 6s 8ms/step - loss: 0.7052 - accuracy: 0.5057\n",
    "\n",
    "# 5 epoch:\n",
    "# 782/782 [==============================] - 6s 8ms/step - loss: 0.6328 - accuracy: 0.7213"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot model\n",
    "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do single prediction\n",
    "# predict probabilities for test set first exemple\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "print(yhat_probs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# understand the dataset\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_index.get(i, '?') for i in text])\n",
    "for i in range(5):\n",
    "    review = decode_review(x_train[i])\n",
    "    print(review)\n",
    "    print(\"Sentiment:\")\n",
    "    print(y_train[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
