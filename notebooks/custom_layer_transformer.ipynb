{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-17T20:37:14.176914Z",
     "start_time": "2023-06-17T20:37:14.170273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple implementation of a custom layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, layers, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
    "\n",
    "# pad the sequences to the same length\n",
    "x_train = pad_sequences(x_train, maxlen=200)\n",
    "x_test = pad_sequences(x_test, maxlen=200)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T20:37:16.863589Z",
     "start_time": "2023-06-17T20:37:14.792865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0  # check if embed_dim is divisible by num_heads\n",
    "\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.embed_dim // self.num_heads))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, embed_dim/num_heads)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, embed_dim/num_heads)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, embed_dim/num_heads)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, embed_dim/num_heads)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T20:37:17.455201Z",
     "start_time": "2023-06-17T20:37:17.444786Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When creating a custom layer in TensorFlow, there are a few methods that you will need to implement and some that are optional but can provide extra functionality if needed.\n",
    "\n",
    "Here's a brief overview of them:\n",
    "\n",
    "init(): This method is called when the layer is created. It's used to initialize the layer and its parameters.\n",
    "build(input_shape): This method is used to create the layer's weights. The input_shape parameter is a tuple that contains the shape of the input data the layer will process. This method is the place where you can add weights that depend on the input shape.\n",
    "**call(inputs, kwargs): This is where the layer's logic lives. This method processes the inputs and applies operations using the weights initialized in the build method.\n",
    "The above methods are pretty much required for every custom layer. However, there are some optional methods that you might find useful:\n",
    "\n",
    "get_config(): This method is used to save the configuration of a layer. It returns a Python dictionary containing the configuration of the layer. The dictionary includes the layer's name, its parameters, and its weights.\n",
    "from_config(config): This method takes the output of get_config() to create a new layer instance that's identical to the one the configuration was saved from.\n",
    "compute_output_shape(input_shape): This method is used to compute the output shape of the layer given the input shape. It is useful when you want to stack multiple layers together and need to check if the output shape of one layer matches the input shape of the next layer.\n",
    "compute_mask(inputs, mask=None): This is used when your layer modifies the mask or creates a new one. By default, this is just pass through for any masks.\n",
    "Remember that the most important methods are build() and call(). Most of the time, you'll just need to implement these two methods to create a custom layer in TensorFlow\n",
    ".\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.3591 - accuracy: 0.8374 - val_loss: 0.2947 - val_accuracy: 0.8778\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1617 - accuracy: 0.9406 - val_loss: 0.3520 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1698b07c0>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "# Find the maximum word index in x_train and x_test\n",
    "max_word_index = max(np.max(x_train), np.max(x_test))\n",
    "# Set vocab_size to be at least as large as max_word_index + 1\n",
    "vocab_size = max_word_index + 1  # Add 1 because word indices start from 1\n",
    "max_sequence_length = x_train.shape[1]  # The length of each sequence\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "attention_layer = MultiHeadSelfAttention(embed_dim, num_heads)(embedding_layer)\n",
    "pooling_layer = layers.GlobalAveragePooling1D()(attention_layer)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T20:46:22.926138Z",
     "start_time": "2023-06-17T20:45:40.428479Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3520 - accuracy: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.3519848585128784, 0.8653600215911865]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss of 0.0788\n",
    "# Evaluating the model\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "# 782/782 [==============================] - 6s 7ms/step - loss: 0.3520 - accuracy: 0.8654"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T20:46:44.846977Z",
     "start_time": "2023-06-17T20:46:39.235221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
